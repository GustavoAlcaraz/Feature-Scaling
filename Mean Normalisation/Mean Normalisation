Mean Normalisation
Mean normalisation involves centering the variable at zero, and re-scaling to the value range. 
The procedure involves subtracting the mean of each observation and then dividing by difference 
between the minimum and maximum value:

x_scaled = (x - x_mean) / ( x_max - x_min)

The result of the above transformation is a distribution that is centered at 0, and its minimum 
and maximum values are within the range of -1 to 1. The shape of a mean normalised distribution 
will be very similar to the original distribution of the variable, but the variance may change, 
so not identical.

Again, this technique will not normalize the distribution of the data thus if this is the desired 
outcome, we should implement any of the techniques discussed in variable transformation.

In a nutshell, mean normalisation:

centers the mean at 0
variance will be different
may alter the shape of the original distribution
the minimum and maximum values squeezed between -1 and 1
preserves outliers
Good for algorithms that require features centered at zero.

In this demo
There is no Scikit-learn transformer for mean normalisation, but we can implement it using a 
combination of 2 other transformers that I will discuss in detail in the next notebooks. 
We will also implement it manually with pandas.
