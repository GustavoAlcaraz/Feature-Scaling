Standardisation
Standardisation involves centering the variable at zero, and standardising the variance to 1. 
The procedure involves subtracting the mean of each observation and then dividing by the standard 
deviation:

z = (x - x_mean) / std

The result of the above transformation is z, which is called the z-score, and represents how 
many standard deviations a given observation deviates from the mean. A z-score specifies the 
location of the observation within a distribution (in numbers of standard deviations respect 
to the mean of the distribution). The sign of the z-score (+ or - ) indicates whether the 
observation is above (+) or below ( - ) the mean.

The shape of a standardised (or z-scored normalised) distribution will be identical to the original 
distribution of the variable. If the original distribution is normal, then the standardised 
distribution will be normal. But, if the original distribution is skewed, then the standardised 
distribution of the variable will also be skewed. In other words, standardising a variable does 
not normalize the distribution of the data and if this is the desired outcome, we should 
implement any of the techniques discussed in variable transformation.

In a nutshell, standardisation:

centers the mean at 0
scales the variance at 1
preserves the shape of the original distribution
the minimum and maximum values of the different variables may vary
preserves outliers
Good for algorithms that require features centered at zero.

In this demo
We will perform standardisation using Scikit-learn
